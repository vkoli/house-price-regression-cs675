{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b574094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('E:/Nehali/MS DS/Machine Learning - Daming Li/Nehali/Midterm/house-prices-advanced-regression-techniques/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc73481",
   "metadata": {},
   "source": [
    "### First let's looks at the summary for numerical data to get a sense of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf58ecd",
   "metadata": {},
   "source": [
    "### We plot a heatmap to measure multicollinearity in our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(abs(train.drop('Id', axis=1).corr()), cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d035ff",
   "metadata": {},
   "source": [
    "### Some features pairs are so highly correlated, we can drop one from our data to improve our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_num = ['GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'TotRmsAbvGrd']\n",
    "train = train.drop(remove_num,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd1999",
   "metadata": {},
   "source": [
    "### Now, let's examine categorical features by plotting their distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04485f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = train.columns[train.dtypes=='object']\n",
    "f, ax = plt.subplots(10, 5, figsize=(50, 50))\n",
    "for i, c in enumerate(categorical_vars):\n",
    "    g = sns.barplot(data=pd.DataFrame(train[c].value_counts()).reset_index(), x='index', y=c, ax=ax[i//5, i%5])\n",
    "    g.set(xticks=[])\n",
    "    g.set(title=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce1963",
   "metadata": {},
   "source": [
    "#### We can probably drop features with one overwhelmingly common category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cat = ['Street', 'LandContour', 'Utilities', \n",
    "              'LandSlope', 'Condition1', 'Condition2', \n",
    "              'RoofMatl','BsmtCond', 'BsmtFinType2', \n",
    "              'Heating', 'GarageCond', 'GarageQual', 'Functional'] \n",
    "train = train.drop(remove_cat, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefa33c",
   "metadata": {},
   "source": [
    "### Now, let's do one-hot encodings for our categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False,drop='first')\n",
    "data_obj = ohe.fit_transform(train[train.columns[train.dtypes=='object']])\n",
    "\n",
    "obj_df = pd.DataFrame(data_obj,columns=ohe.get_feature_names(train.columns[train.dtypes=='object']))\n",
    "cat_columns = obj_df.columns\n",
    "\n",
    "train = pd.concat([train[train.columns[train.dtypes!='object']], \n",
    "                         obj_df],\n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45333864",
   "metadata": {},
   "source": [
    "### Let's check for any low variance features and drop them:\n",
    "#### First we must normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2638a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = (train-train.mean())/train.std()\n",
    "train=(train-train.min())/(train.max()-train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14aca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_var = train.columns[train.var()<.001]\n",
    "print(low_var)\n",
    "train = train.drop(columns=low_var,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1263aa5",
   "metadata": {},
   "source": [
    "### In order to run models, we must remove any Nan values:\n",
    "#### First let's check which features have Nans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938efd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns[train.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044605dc",
   "metadata": {},
   "source": [
    "#### In these 2 cases, we can interpret any Nan values as 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c354380",
   "metadata": {},
   "source": [
    "### Finding the best features can be done in numerous ways, we went with 3:\n",
    "### - Extracting feature importance through Random Forest model\n",
    "### - Checking correlation with target feature\n",
    "### - Discussing real-life importance of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a3958",
   "metadata": {},
   "source": [
    "#### Random Forest analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92309928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = train.drop(['Id', 'SalePrice'], axis=1)\n",
    "y = train['SalePrice']\n",
    "\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "model.fit(X,y)\n",
    "\n",
    "features = X.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-10:]  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb6e63",
   "metadata": {},
   "source": [
    "### Now, let's examine the best correlations with the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_vals = train.corr(method='pearson')\n",
    "sorted_corr_vals = corr_vals['SalePrice'].sort_values(ascending=False)\n",
    "temp = sorted_corr_vals.to_frame(name='corr')\n",
    "temp.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadca0c",
   "metadata": {},
   "source": [
    "### These methods, along with group discussion on real-world impacts these features likely have on sale-price, we arrived at the following features to try:\n",
    "- Overall Quality \n",
    "- GrLivArea\n",
    "- LotArea \n",
    "- GarageCars\n",
    "- FullBath \n",
    "- 1stFlrSF\n",
    "- MasVnrArea \n",
    "- ExterQual\n",
    "- YearBuilt  \n",
    "- Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neighborhood = [col for col in train.columns if 'Neighborhood' in col]\n",
    "ExterQual = [col for col in train.columns if 'ExterQual' in col]\n",
    "\n",
    "features = ['OverallQual', 'GarageCars', 'FullBath', \n",
    "            'YearBuilt', '2ndFlrSF', 'MasVnrArea', \n",
    "            'LotArea', 'GrLivArea'] + Neighborhood + ExterQual\n",
    "\n",
    "train[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b78ce",
   "metadata": {},
   "source": [
    "### Now let's create some new features! One way to do this is to derive features from different columns into a single columns.\n",
    "### For this, we went with: \n",
    "- PorchDeckRank (scaled score of porch and deck quality) \n",
    "- TotRmsAbvGrdwithBath (totrms + full bath + halfbaths) \n",
    "- RelativeAge (YearRemodAdd-YrSold) + MoSold/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('E:/Nehali/MS DS/Machine Learning - Daming Li/Nehali/Midterm/house-prices-advanced-regression-techniques/train.csv')\n",
    "train['RelAge'] = data['YrSold'] - data['YearRemodAdd'] + data['MoSold']/12\n",
    "train['TotRmsAbvGrdBath'] = data[['TotRmsAbvGrd', 'FullBath', 'HalfBath']].sum(axis=1)\n",
    "train['PorchDeckRank'] = data[['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']].sum(axis=1)\n",
    "\n",
    "new_features = ['RelAge','TotRmsAbvGrdBath','PorchDeckRank']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a3d7a",
   "metadata": {},
   "source": [
    "### Another way to add new columns is through PCA analzysis, where we compress variance into fewer columns, reducing dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf050d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "x_pca = pca.fit_transform(train[features])\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4f9ac",
   "metadata": {},
   "source": [
    "### The first two columns contain almost half of the overall variance information of the features, and can be used as extra features in our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PCA1'] = x_pca[[0]]\n",
    "train['PCA2'] = x_pca[[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48703da0",
   "metadata": {},
   "source": [
    "#### Let's normalize these new features and add them to our feature list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features += ['PCA1','PCA2']\n",
    "train[new_features]=(train[new_features]-\n",
    "                     train[new_features].min()\n",
    "                    )/(train[new_features].max()-\n",
    "                       train[new_features].min())\n",
    "\n",
    "features += new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c532d4f",
   "metadata": {},
   "source": [
    "## Now, in order to evaluate our models, let's break our dataset into training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train[features]\n",
    "y = train['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218ff66",
   "metadata": {},
   "source": [
    "#### Let's use the following metrics so we can compare using common metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "# print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "# print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "# print('R-squared Error:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10c60c",
   "metadata": {},
   "source": [
    "### Support Vector Machine for Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='linear', gamma='scale')\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd696cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, svr_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, svr_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))\n",
    "print('R-squared Error:', metrics.r2_score(y_test, svr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba923977",
   "metadata": {},
   "source": [
    "#### Model Improvements - try different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.set_params(C=10.0)\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, svr_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, svr_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))\n",
    "print('R-squared Error:', metrics.r2_score(y_test, svr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.set_params(kernel='poly')\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, svr_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, svr_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))\n",
    "print('R-squared Error:', metrics.r2_score(y_test, svr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1769260",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52f1a3",
   "metadata": {},
   "source": [
    "# KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41aef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = train[features]\n",
    "y = train['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R-squared Error:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43717a",
   "metadata": {},
   "source": [
    "### Evaluating different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = np.arange(1,11)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    knn = KNeighborsRegressor(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "plt.title('KNN Neighbor Number')\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors = 4)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R-squared Error:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_opt = ['uniform', 'distance']\n",
    "train_accuracy = np.empty(len(weights_opt))\n",
    "test_accuracy = np.empty(len(weights_opt))\n",
    "\n",
    "for i in range(len(weights_opt)):\n",
    "    knn = KNeighborsRegressor(n_neighbors = 4, weights = weights_opt[i])\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "plt.title('KNN Weights')\n",
    "plt.plot(weights_opt, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(weights_opt, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Weights')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "train_accuracy = np.empty(len(alg))\n",
    "test_accuracy = np.empty(len(alg))\n",
    "\n",
    "for i in range(len(alg)):\n",
    "    knn = KNeighborsRegressor(n_neighbors = 4, weights = 'distance', algorithm = alg[i])\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "plt.title('KNN Algorithm')\n",
    "plt.plot(alg, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(alg, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf = np.arange(1,100)\n",
    "train_accuracy = np.empty(len(leaf))\n",
    "test_accuracy = np.empty(len(leaf))\n",
    "\n",
    "for i,k in enumerate(leaf):\n",
    "    knn = KNeighborsRegressor(n_neighbors = 4, weights = 'distance', algorithm = 'ball_tree', leaf_size = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "plt.title('KNN Ball Tree Leaf Size')\n",
    "plt.plot(leaf, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(leaf, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Leaf Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf = np.arange(1,100)\n",
    "train_accuracy = np.empty(len(leaf))\n",
    "test_accuracy = np.empty(len(leaf))\n",
    "\n",
    "for i,k in enumerate(leaf):\n",
    "    knn = KNeighborsRegressor(n_neighbors = 4, weights = 'distance', algorithm = 'kd_tree', leaf_size = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "plt.title('KNN KD Tree Leaf Size')\n",
    "plt.plot(leaf, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(leaf, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Leaf Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.arange(1,5)\n",
    "train_accuracy = np.empty(len(p))\n",
    "test_accuracy = np.empty(len(p))\n",
    "\n",
    "for i,k in enumerate(p):\n",
    "    knn = KNeighborsRegressor(n_neighbors = 4, weights = 'distance', algorithm = 'auto', p = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "plt.title('KNN Power Parameter')\n",
    "plt.plot(p, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(p, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Power Parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541832b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors = 4, weights = 'distance', algorithm = 'auto', p = 1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R-squared Error:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ece84",
   "metadata": {},
   "source": [
    "# Overall Accuracy of KNN Model After Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors = 4, weights = 'distance', algorithm = 'auto', p = 1)\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print('Final Accuracy:', str(format(accuracy*100, '.3f')) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275866bf",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23959afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train[features]\n",
    "y = train['SalePrice']\n",
    "\n",
    "lr = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, y_pred),4))  \n",
    "print('Mean Squared Error:', round(metrics.mean_squared_error(y_test, y_pred),4))  \n",
    "print('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4))\n",
    "print('R-squared Error:', round(metrics.r2_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a heatmap to view the features\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(abs(X.corr()), cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ff6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droppig some of the features to improve the model\n",
    "X=X.drop(['ExterQual_Fa','ExterQual_Gd','ExterQual_TA'],axis=1)\n",
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ed78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no improvements found\n",
    "lr = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, y_pred),4))  \n",
    "print('Mean Squared Error:', round(metrics.mean_squared_error(y_test, y_pred),4))  \n",
    "print('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),4))\n",
    "print('R-squared Error:', round(metrics.r2_score(y_test, y_pred), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
